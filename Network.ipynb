{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WVSKl21x4sur"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgQ0Ops35BPq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","import json\n","import h5py\n","import pandas, numpy, random, cv2\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tD50QkfoKak"},"outputs":[],"source":["# Check HDF5 file\n","h5py_dir = '/content/drive/My Drive/Colab Notebooks/Data/train.h5py'\n","\n","with h5py.File(h5py_dir, 'r') as hdf:\n","    if 'in_image' in hdf and 'local' in hdf and 'parameters' in hdf:\n","        print(f\"Number of in_images: {len(hdf['in_image'])}\")\n","        print(f\"Number of local: {len(hdf['local'])}\")\n","        print(f\"Number of parameters: {len(hdf['parameters'])}\")\n","    else:\n","        print(\"Expected datasets not found in the HDF5 file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vdC7lSYx-ou"},"outputs":[],"source":["# Check if CUDA is available. If yes, set default tensor type to cuda\n","\n","if torch.cuda.is_available():\n","    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","    print(\"using cuda:\", torch.cuda.get_device_name(0))\n","    pass\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEZgbZpkVzWS"},"outputs":[],"source":["class View(nn.Module):\n","    # A variation of .view that can be exectued inside nn.Sequential()\n","    def __init__(self, shape):\n","        super().__init__()\n","        self.shape = shape\n","\n","    def forward(self, x):\n","        return x.view(self.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3FlFi-CSr7A"},"outputs":[],"source":["def in_painting(in_image, coordinate, left_patch, right_patch):\n","\n","    patched_image = in_image\n","\n","    # Resize left eye\n","    lx, ly = int(coordinate[0].item()), int(coordinate[1].item())\n","    lw, lh = int(coordinate[2].item()), int(coordinate[3].item())\n","    up = max(0, int(ly - lh / 2))\n","    down = min(80, int(ly + lh / 2) + 1)\n","    lh = down - up\n","    left_patch_resized = cv2.resize(left_patch, (lw + 1, lh))\n","\n","    # In paint the left eye\n","    patched_image[up:down, int(lx - lw / 2):(int(lx - lw / 2) + lw + 1)] = left_patch_resized\n","\n","    # Resize the right eye\n","    rx, ry = int(coordinate[4].item()), int(coordinate[5].item())\n","    rw, rh = int(coordinate[6].item()), int(coordinate[7].item())\n","    up = max(0, int(ry - rh / 2))\n","    down = min(80, int(ry + rh / 2) + 1)\n","    rh = down - up\n","    right_patch_resized = cv2.resize(right_patch, (rw + 1, rh))\n","\n","    # In paint the right eye\n","    patched_image[up:down, int(rx - rw / 2):(int(rx - rw / 2) + rw + 1)] = right_patch_resized\n","\n","    return patched_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWyBUwkitWb5"},"outputs":[],"source":["class Celeb_ID_Dataset(Dataset):\n","\n","    def __init__(self, file_path):\n","        self.file_path = file_path\n","        self.file_object = h5py.File(file_path, 'r')\n","        self.data_len = len(self.file_object['in_image'])\n","\n","    def __len__(self):\n","        return self.data_len\n","\n","    def __getitem__(self, idx):\n","        in_image = self.file_object['in_image'][idx]\n","        local = self.file_object['local'][idx]\n","        parameter = self.file_object['parameters'][idx]\n","        left = local[0:100, 0:100, ...]\n","        right = local[0:100, 100:200, ...]\n","        ur_image = in_painting(in_image, parameter, left, right)\n","\n","        in_image_ = torch.FloatTensor(in_image).permute(2,0,1).view(3, 80, 256) / 255.0\n","        local_ = torch.FloatTensor(local).permute(2,0,1).view(3, 100, 200) / 255.0\n","        param_ = torch.FloatTensor(parameter)\n","        ur_image_ = torch.FloatTensor(ur_image).permute(2,0,1).view(3, 80, 256) / 255.0\n","\n","        return in_image_, local_, param_, ur_image_\n","\n","    def __del__(self):\n","        self.file_object.close()\n","\n","# Create Dataset and Dataloader object\n","file_path = '/content/drive/My Drive/Colab Notebooks/Data/train.h5py'\n","dataset = Celeb_ID_Dataset(file_path)\n","params = {'batch_size': 32, 'shuffle': True, 'generator':torch.Generator(device='cuda')}\n","dataloader = DataLoader(dataset, **params)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YmK2TuaKqgXP"},"outputs":[],"source":["# @title\n","# Check dataset and dataloader\n","\n","print(\"Length of the dataset:\", len(dataset))\n","\n","def show_batch_images(images, batch_size):\n","    fig, axes = plt.subplots(4, 8, figsize=(15, 7))\n","    axes = axes.flatten()\n","\n","    for img, ax in zip(images, axes):\n","        img = img.permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","        ax.imshow(img, cmap='gray')\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","try:\n","    data_iter = iter(dataloader)\n","    first_batch = next(data_iter)\n","    print(\"\\nDataLoader contains data.\")\n","\n","    in_image, local, parameters = first_batch\n","    print(\"\\nExample of a batch of parameters:\")\n","    print(parameters)\n","    print(\"\\nExample of a batch of images:\")\n","    # show_batch_images(in_image, batch_size=32)\n","    show_batch_images(local, batch_size=32)\n","\n","except StopIteration:\n","    print(\"DataLoader is empty.\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mf5ea_QZH6tR"},"outputs":[],"source":["class Discriminator(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Global Discriminator\n","        self.global_discriminator = nn.Sequential(\n","            # The input should be a 80 * 256 * 3 image that has been patched\n","            nn.Conv2d(3, 128, kernel_size=(9, 27), stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 64, kernel_size=(7, 27), stride=2),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 3, kernel_size=(1, 6), stride=2),\n","            nn.BatchNorm2d(3),\n","            nn.LeakyReLU(0.2),\n","            View((32, 3*8*20)),\n","            nn.Linear(3*8*20, 100)\n","            # The output should be a 1D tensor with 100 elements\n","        )\n","\n","        # Local Discriminator\n","        self.local_discriminator = nn.Sequential(\n","            # The input should be a 100 * 200 * 3 image\n","            nn.Conv2d(3, 128, kernel_size=5, stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 64, kernel_size=5, stride=2),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 3, kernel_size=(4, 8), stride=2),\n","            nn.BatchNorm2d(3),\n","            nn.LeakyReLU(0.2),\n","            View((32, 3*10*20)),\n","            nn.Linear(3*10*20, 100)\n","            # The output should be a 1D tensor with 100 elements\n","        )\n","\n","        # The final layer with one neuron whose output is a scalar and a Sigmoid activation\n","        self.concat_fc = nn.Sequential(\n","            nn.Linear(200, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.optimiser = torch.optim.Adam(self.parameters(), lr=0.0001)\n","\n","        self.counter = 0\n","        self.progress = []\n","        pass\n","\n","    def forward(self, global_x, local_x):\n","        # Concatenate the global and local discriminator outputs\n","        global_output = self.global_discriminator(global_x)\n","        local_output = self.local_discriminator(local_x)\n","        output = torch.cat((global_output, local_output), dim=1)\n","        output = self.concat_fc(output)\n","        return output\n","\n","    def cal_bce_loss(self, d_score, label):\n","        \"\"\"\n","        Args:\n","            d_score: Output of the global and local discriminators.\n","            label: L'Ã©tiquette.\n","        \"\"\"\n","        # BCE loss of the concatenated local and global discriminator\n","        alpha = 5e-4\n","        d_loss = F.binary_cross_entropy(d_score, label)\n","        return alpha * d_loss\n","\n","    def cal_mse_loss(self, local_x, ur_local):\n","        # MSE loss of the global image\n","        local_loss = F.mse_loss(local_x, ur_local, reduction='mean')\n","        return local_loss\n","\n","    def train_on_true(self, global_x, local_x):\n","        # Calculate d_score and the total loss\n","        d_score = self.forward(global_x, local_x)\n","        label = torch.ones(d_score.shape)\n","        loss = self.cal_bce_loss(d_score, label)\n","\n","        # increase counter and accumulate error every 10\n","        self.counter += 1;\n","        if (self.counter % 10 == 0):\n","            self.progress.append(loss.item())\n","            pass\n","        if (self.counter % 1000 == 0):\n","            print(\"counter = \", self.counter)\n","            pass\n","\n","        # Reset gradients, perform back propagation and update weights\n","        self.optimiser.zero_grad()\n","        loss.backward()\n","        self.optimiser.step()\n","        pass\n","\n","    def train_on_fake(self, global_x, local_x, ur_local):\n","        # Calculate d_score and the total loss\n","        d_score = self.forward(global_x, local_x)\n","        label = torch.zeros(d_score.shape)\n","        bce_loss = self.cal_bce_loss(d_score, label)\n","        mse_loss = self.cal_mse_loss(local_x, ur_local)\n","\n","        loss = bce_loss + mse_loss\n","\n","        # increase counter and accumulate error every 10\n","        self.counter += 1;\n","        if (self.counter % 10 == 0):\n","            self.progress.append(loss.item())\n","            pass\n","        if (self.counter % 1000 == 0):\n","            print(\"counter = \", self.counter)\n","            pass\n","\n","        # Reset gradients, perform back propagation and update weights\n","        self.optimiser.zero_grad()\n","        loss.backward()\n","        self.optimiser.step()\n","        pass\n","\n","    def plot_progress(self):\n","        df = pandas.DataFrame(self.progress, columns=['loss'])\n","        df.plot(ylim=(0), figsize=(16,8), alpha=0.1, marker='.', grid=True, yticks=(0, 0.1, 0.02))\n","        pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ch17hOknmbH5"},"outputs":[],"source":["class Generator(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.concentration = nn.Sequential(\n","            # The input should be a 1 * 3 * 80 * 256 image\n","            nn.Conv2d(3, 128, kernel_size=(4, 27), stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 128, kernel_size=(3, 27), stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 3, kernel_size=(1, 7), stride=2),\n","            nn.BatchNorm2d(3),\n","            nn.LeakyReLU(0.2)\n","            # This gives a convolved image of shape 1 * 3 * 10 * 20\n","        )\n","\n","        self.model = nn.Sequential(\n","            # The input should be a 1 * 3 * 10 * 20 tensor\n","            nn.ConvTranspose2d(3, 64, kernel_size=5, stride=2),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(64, 128, kernel_size=5, stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(128, 3, kernel_size=(4, 24), stride=2),\n","            nn.BatchNorm2d(3),\n","            nn.Sigmoid()\n","            # The output should be a 1 * 3 * 100 * 200 image of the in painting region\n","        )\n","\n","        self.optimiser = torch.optim.Adam(self.parameters(), lr=0.0001)\n","\n","        self.counter = 0\n","        self.progress = []\n","        pass\n","\n","    def forward(self, inputs):\n","        convolved_image = self.concentration(inputs)\n","        # noise = torch.randn(32, 3, 2, 20)\n","        # Add a noise tensor of shape 2 * 20 * 3 vertically\n","        # concatenated_tensor = torch.cat((convolved_image, noise), dim=2)\n","        return self.model(convolved_image)\n","\n","    def train(self, D, global_x, local_x, ur_local):\n","        # Calculate d_score and the total loss\n","        d_score = D.forward(global_x, local_x)\n","        label = torch.ones(d_score.shape)\n","        bce_loss = D.cal_bce_loss(d_score, label)\n","        mse_loss = D.cal_mse_loss(local_x, ur_local)\n","\n","        loss = bce_loss + mse_loss\n","\n","        # Increase counter and accumulate error every 10\n","        self.counter += 1\n","        if (self.counter % 10 == 0):\n","            self.progress.append(loss.item())\n","            pass\n","\n","        # Reset gradients, perform back propagation and update weights\n","        self.optimiser.zero_grad()\n","        loss.backward()\n","        self.optimiser.step()\n","        pass\n","\n","    def plot_progress(self):\n","        df = pandas.DataFrame(self.progress, columns=['loss'])\n","        df.plot(ylim=(0), figsize=(16,8), alpha=0.1, marker='.', grid=True, yticks=(0, 0.1, 0.2))\n","        pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"z1XSW3vmvB3z"},"outputs":[],"source":["# @title\n","# Test Discriminator\n","\n","D = Discriminator().to(device)\n","G = Generator().to(device)\n","\n","for batch_in_image, batch_ur_local, batch_parameter in dataloader:\n","    # Move data to CUDA device\n","    batch_in_image = batch_in_image.to(device)\n","    batch_ur_local = batch_ur_local.to(device)\n","    batch_parameter = batch_parameter.to(device)\n","\n","    # Train the discriminator on true\n","    D.train_on_true(batch_in_image, batch_ur_local)\n","\n","    # Train the discriminator on false\n","    batch_fake_image = G.forward(batch_in_image).detach()\n","    # Patch the image\n","    patched = []\n","    for i in range(batch_fake_image.size(0)):\n","        in_image = batch_in_image[i].unsqueeze(0)\n","        raw_patches = batch_fake_image[i].unsqueeze(0)\n","        parameter = batch_parameter[i].unsqueeze(1)\n","\n","        in_img = in_image.permute(0,2,3,1).view(80,256,3).cpu().numpy()\n","        gray_patches = raw_patches.permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","        patches = (gray_patches * 255).clip(0, 255).astype(numpy.uint8)\n","        coordinate = parameter.cpu().numpy()\n","\n","        left_patch = patches[0:100, 0:100, ...]\n","        right_patch = patches[0:100, 100:200, ...]\n","        completion = in_painting(in_img, coordinate, left_patch, right_patch)\n","        patched.append(torch.FloatTensor(completion).permute(2,0,1).view(3, 80, 256) / 255.0)\n","\n","    # batch_patched = torch.cat(patched, dim=0).to(device)\n","    batch_patched = torch.stack(patched).to(device)\n","    print(batch_patched.shape)\n","\n","    # Train the discriminator on false\n","    fake_image = G.forward(batch_in_image).detach()\n","    D.train_on_fake(batch_patched, batch_fake_image, batch_ur_local)\n","\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LjK_UjLQZhu"},"outputs":[],"source":["def process_image(batch_in_image, batch_fake_image, batch_parameter):\n","    # Unpack the batch, perform individual operation on each and repack in to batch\n","    patched = []\n","    for i in range(batch_fake_image.size(0)):\n","        in_image = batch_in_image[i].unsqueeze(0)\n","        raw_patches = batch_fake_image[i].unsqueeze(0)\n","        parameter = batch_parameter[i].unsqueeze(1)\n","\n","        in_img = in_image.permute(0,2,3,1).view(80,256,3).cpu().numpy()\n","        gray_patches = raw_patches.detach().permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","        patches = (gray_patches).clip(0, 1).astype(numpy.float32)\n","        coordinate = parameter\n","\n","        left_patch = patches[0:100, 0:100, ...]\n","        right_patch = patches[0:100, 100:200, ...]\n","        completion = in_painting(in_img, coordinate, left_patch, right_patch)\n","        patched.append(torch.FloatTensor(completion).permute(2,0,1).view(3, 80, 256))\n","\n","    # Re-pack to mini-batch\n","    batch_patched = torch.stack(patched).to(device)\n","    return batch_patched"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fff592kS9yU"},"outputs":[],"source":["# Create Discriminator and Generator\n","D = Discriminator().to(device)\n","G = Generator().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQUOlh-32Pu7"},"outputs":[],"source":["# Load parameters (opt)\n","D.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/D_06042237_30.pth'))\n","G.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/G_06042237_30.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TFGVQBpfAXx"},"outputs":[],"source":["%%time\n","\n","# Train the Network\n","\n","current_epoch = 0\n","epoch = 5\n","batch_size = 32\n","\n","for epoch in range(epoch):\n","    current_epoch += 1\n","    print(\"epoch = \", current_epoch)\n","\n","    for batch_in_image, batch_ur_local, batch_parameter, batch_ur_image in dataloader:\n","        # Move data to CUDA device\n","        gpu_in_image = batch_in_image.to(device)\n","        gpu_ur_local = batch_ur_local.to(device)\n","        gpu_ur_image = batch_ur_image.to(device)\n","\n","        # Train the discriminator on true *\n","        D.train_on_true(gpu_ur_image, gpu_ur_local)\n","\n","        # In paint with noise tensor\n","        noise = (0.01 * torch.randn(3, 100, 200)).to(device)\n","        batch_noise = noise.unsqueeze(0).repeat(32, 1, 1, 1)\n","        gpu_input = process_image(batch_in_image, batch_noise, batch_parameter)\n","\n","        # Generate patches and global for the next trainings\n","        gpu_g_image = G.forward(gpu_input)\n","        gpu_patched = process_image(batch_in_image, gpu_g_image, batch_parameter)\n","\n","        # Train the discriminator on fake (false) *\n","        gpu_fake_image = gpu_g_image.detach()\n","        D.train_on_fake(gpu_patched, gpu_fake_image, gpu_ur_local)\n","\n","        # Train the generator *\n","        G.train(D, gpu_patched, gpu_g_image, gpu_ur_local)\n","\n","torch.save(D.state_dict(), '/content/drive/My Drive/Colab Notebooks/D_ep_5.pth')\n","torch.save(G.state_dict(), '/content/drive/My Drive/Colab Notebooks/G_ep_5.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvQ-xvXxgS8H"},"outputs":[],"source":["torch.save(D.state_dict(), '/content/drive/My Drive/Colab Notebooks/D_06050148_35.pth')\n","torch.save(G.state_dict(), '/content/drive/My Drive/Colab Notebooks/G_06050148_35.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeubpRKd_Rmp"},"outputs":[],"source":["# Check the result after training\n","\n","fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n","\n","for batch_in_image, batch_ur_local, batch_parameter, batch_ur_image in dataloader:\n","    # Move data to CUDA device\n","    batch_in_image = batch_in_image.to(device)\n","    batch_ur_local = batch_ur_local.to(device)\n","    batch_parameter = batch_parameter.to(device)\n","\n","    batch_patch = G.forward(batch_in_image).detach()\n","\n","    for i in range(batch_patch.size(0)):\n","        in_image = batch_in_image[i].unsqueeze(0)\n","        raw_patches = batch_patch[i].unsqueeze(0)\n","        ur_patches = batch_ur_local[i].unsqueeze(0)\n","        parameter = batch_parameter[i].unsqueeze(1)\n","\n","        in_img = in_image.permute(0,2,3,1).view(80,256,3).cpu().numpy()\n","        real_patch = ur_patches.permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","        gray_patches = raw_patches.permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","        patches = (gray_patches).clip(0, 1).astype(numpy.float32)\n","        coordinate = parameter.cpu().numpy()\n","\n","        axs[0].imshow(patches, cmap='gray')\n","        axs[0].set_title(\"Patch\")\n","\n","        axs[1].imshow(real_patch, cmap='gray')\n","        axs[1].set_title(\"Original\")\n","\n","        left_patch = patches[0:100, 0:100, ...]\n","        right_patch = patches[0:100, 100:200, ...]\n","        completion = in_painting(in_img, parameter, left_patch, right_patch)\n","\n","        axs[2].imshow(in_img, cmap='gray')\n","        axs[2].set_title(\"Input\")\n","\n","        axs[3].imshow(completion.clip(0, 1).astype(numpy.float32), cmap='gray')\n","        axs[3].set_title(\"Completion\")\n","\n","        break\n","\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2Jp72jyBcDE"},"outputs":[],"source":["# Plot discriminator error\n","D.plot_progress()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZf-ezSWBc37"},"outputs":[],"source":["# Plot generator error\n","G.plot_progress()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNySjK3phRK4"},"outputs":[],"source":["# Application\n","\n","fig, axes = plt.subplots(1, 2, figsize=(9, 6))\n","\n","input_image = cv2.imread('/content/drive/My Drive/Colab Notebooks/Data/ts.jpg')\n","input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n","in_image = input_image[0:80, 0:256, ...]\n","ur_image = input_image[80:160, 0:256, ...]\n","\n","parameter = numpy.array([[81,], [41,], [53,], [48,], [179,], [40,], [48,], [49,]])\n","\n","gallery = []\n","gallery.append(ur_image)\n","\n","input = torch.FloatTensor(in_image).permute(2,0,1).view(3, 80, 256) / 255.0\n","input = input.to(device)\n","batch_input = input.unsqueeze(0).repeat(32, 1, 1, 1)\n","batch_patches = G.forward(batch_input).detach()\n","\n","for i in range(1):\n","    raw_patches = batch_patches[i].unsqueeze(0)\n","    gray_patches = raw_patches.permute(0,2,3,1).view(100,200,3).cpu().numpy()\n","    patches = (gray_patches * 255).clip(0, 255).astype(numpy.uint8)\n","\n","    left_patch = patches[0:100, 0:100, ...]\n","    right_patch = patches[0:100, 100:200, ...]\n","\n","    completion = in_painting(in_image, parameter, left_patch, right_patch)\n","    gallery.append(completion)\n","    pass\n","\n","\n","axes = axes.flatten()\n","num = 0\n","for img, ax in zip(gallery, axes):\n","    ax.imshow(img)\n","    if num == 0:\n","        ax.set_title(\"Original\")\n","    else:\n","        ax.set_title(f\"Completion {num}\")\n","    ax.axis('off')\n","    num += 1\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[],"authorship_tag":"ABX9TyPuwNKbKkLEal3H+Sdk6/gl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}